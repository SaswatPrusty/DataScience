{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lending is one of the critical functions of any bank or financial institution. Customers are provided loans across different products (home loan, loan against property etc.) at competitive interest rates for an acceptable tenure. There is always a risk that a customer may default on the loan or may try and repay the loan in advance which leads to financial losses to the business.  \n",
    "\n",
    "There may not be enough data or evidence available from the past which will help the firm to predict the loan default or prepayment and mitigate the above possible risk. However, it does have information about the customer demographics, loan details, EMI transactions etc. which contributes to more than 25-30 features (high-dimension data) related to a customerâ€™s loan account. But due to high dimensions it is difficult to identify any patterns within the data. Unsupervised learning techniques like Principal Component Analysis comes to rescue here to reduce the high dimensionality and help in further analysis and pattern recognition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data file contains information about the loan transactions done by the customer. \n",
    "\n",
    "All important details regarding the loan i.e. loan amount, interest rate, outstanding principal, loan to value ratio (Net LTV), tenure, city where the loan was originated etc. has been provided. \n",
    "\n",
    "Some of the features that correspond to multiple loan transactions (e.g. rate of interest, emi amount, frequency of emi payment etc.) for a loan account are summarized and captured for each loan account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import  required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting factor_analyzer\n",
      "  Using cached factor_analyzer-0.3.2.tar.gz (40 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\anaconda3\\lib\\site-packages (from factor_analyzer) (1.1.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from factor_analyzer) (1.5.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from factor_analyzer) (1.19.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\anaconda3\\lib\\site-packages (from factor_analyzer) (0.23.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->factor_analyzer) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->factor_analyzer) (2.8.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->factor_analyzer) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->factor_analyzer) (2.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->factor_analyzer) (1.15.0)\n",
      "Building wheels for collected packages: factor-analyzer\n",
      "  Building wheel for factor-analyzer (setup.py): started\n",
      "  Building wheel for factor-analyzer (setup.py): finished with status 'done'\n",
      "  Created wheel for factor-analyzer: filename=factor_analyzer-0.3.2-py3-none-any.whl size=40383 sha256=f402d4da4d4bf8fa02f8d4a23690860ca92be1a56a516ada7523b3f2f64709de\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\b1\\d4\\b5\\9da0e0e81266e87f5e85068f031077c8a2ae0eedf76ea1d294\n",
      "Successfully built factor-analyzer\n",
      "Installing collected packages: factor-analyzer\n",
      "Successfully installed factor-analyzer-0.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install factor_analyzer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from factor_analyzer import FactorAnalyzer # Perform statistical tests before PCA \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and view data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.city.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.marital_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.qualification.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.city.replace(to_replace='Mum',value='MUMBAI',inplace=True)\n",
    "df.city.replace(to_replace='Hyd',value='HYDERABAD',inplace=True)\n",
    "df.city.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['diff_current_interest_rate_max_min','diff_original_current_interest_rate','diff_original_current_tenor','excess_adjusted_amt'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.gross_income.replace(to_replace=0, value=df.gross_income.median(),inplace=True)\n",
    "df.nettakehome.replace(to_replace=0, value=df.nettakehome.median(),inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariateAnalysis_numeric(column,nbins):\n",
    "    print(\"Description of \" + column)\n",
    "    print(\"----------------------------------------------------------------------------\")\n",
    "    print(df[column].describe(),end=' ')\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "    print(\"Distribution of \" + column)\n",
    "    print(\"----------------------------------------------------------------------------\")\n",
    "    sns.distplot(df[column], kde=False, color='g');\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    print(\"BoxPlot of \" + column)\n",
    "    print(\"----------------------------------------------------------------------------\")\n",
    "    ax = sns.boxplot(x=df[column])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univariateAnalysis_numeric('age',20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = df.select_dtypes(include = ['float64', 'int64'])\n",
    "lstnumericcolumns = list(df_num.columns.values)\n",
    "len(lstnumericcolumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in lstnumericcolumns:\n",
    "    univariateAnalysis_numeric(x,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.num_low_freq_transactions.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probable customers who are paying off their loans earlier\n",
    "df[df.num_low_freq_transactions > 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariateAnalysis_category(cat_column):\n",
    "    print(\"Details of \" + cat_column)\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(df_cat[cat_column].value_counts())\n",
    "    plt.figure()\n",
    "    df_cat[cat_column].value_counts().plot.bar(title=\"Frequency Distribution of \" + cat_column)\n",
    "    plt.show()\n",
    "    print(\"       \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df.select_dtypes(include = ['object'])\n",
    "lstcatcolumns = list(df_cat.columns.values)\n",
    "lstcatcolumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstcatcolumns.remove('loan_account')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in lstcatcolumns:\n",
    "    univariateAnalysis_category(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_num.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.triu(np.ones_like(corr, dtype=np.bool)) \n",
    "fig = plt.subplots(figsize=(25, 15))\n",
    "sns.heatmap(df_num.corr(), annot=True,fmt='.2f',mask=mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num.boxplot(figsize=(20,20))\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_outlier = ['age','current_interest_rate','current_interest_rate_max','current_interest_rate_min','net_ltv','orignal_interest_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num[no_outlier].boxplot(figsize=(20,20))\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To treat outliers lets define a function **'treat_outlier'**. \n",
    "\n",
    "- For the higher outliers we will treat it to get it at 95 percentile value. \n",
    "\n",
    "- Lower level outliers will be treated to get it at 5 percentile value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treat_outlier(x):\n",
    "    # taking 5,25,75 percentile of column\n",
    "    q5= np.percentile(x,5)\n",
    "    q25=np.percentile(x,25)\n",
    "    q75=np.percentile(x,75)\n",
    "    dt=np.percentile(x,95)\n",
    "    #calculationg IQR range\n",
    "    IQR=q75-q25\n",
    "    #Calculating minimum threshold\n",
    "    lower_bound=q25-(1.5*IQR)\n",
    "    upper_bound=q75+(1.5*IQR)\n",
    "    #Capping outliers\n",
    "    return x.apply(lambda y: dt if y > upper_bound else y).apply(lambda y: q5 if y < lower_bound else y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_list = [x for x in df_num.columns if x not in no_outlier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_num[outlier_list]:    \n",
    "    df_num[i]=treat_outlier(df_num[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num.boxplot(figsize=(20,20))\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "df_num_scaled=df_num.apply()\n",
    "df_num_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_scaled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_scaled.boxplot(figsize=(20,3))\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical tests to be done before PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bartletts Test of Sphericity\n",
    "Bartlett's test of sphericity tests the hypothesis that the variables are uncorrelated in the population.\n",
    "\n",
    "- H0: All variables in the data are uncorrelated\n",
    "- Ha: At least one pair of variables in the data are correlated\n",
    "\n",
    "If the null hypothesis cannot be rejected, then PCA is not advisable.\n",
    "\n",
    "If the p-value is small, then we can reject the null hypothesis and agree that there is atleast one pair of vairbales in the data wihich are correlated hence PCA is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "chi_square_value,p_value=calculate_bartlett_sphericity(df_num_scaled)\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMO Test\n",
    "\n",
    "The Kaiser-Meyer-Olkin (KMO) - measure of sampling adequacy (MSA) is an index used to examine how appropriate PCA is.\n",
    "\n",
    "Generally, if MSA is less than 0.5, PCA is not recommended, since no reduction is expected. On the other hand, MSA > 0.7 is expected to provide a considerable reduction is the dimension and extraction of meaningful components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "kmo_all,kmo_model=calculate_kmo(df_num_scaled)\n",
    "kmo_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1- Create the covariance Matrix\n",
    "cov_matrix = np.cov(df_num_scaled.T)\n",
    "print('Covariance Matrix \\n%s', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2- Get eigen values and eigen vector\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_matrix)\n",
    "print('\\n Eigen Values \\n %s', )\n",
    "print('\\n')\n",
    "print('Eigen Vectors \\n %s', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T06:29:41.779882Z",
     "start_time": "2020-04-09T06:29:41.759149Z"
    }
   },
   "outputs": [],
   "source": [
    "tot = sum(eig_vals)\n",
    "var_exp = [( i /tot ) * 100 for i in sorted(eig_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "print(\"Cumulative Variance Explained\", cum_var_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scree plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T06:29:42.046671Z",
     "start_time": "2020-04-09T06:29:41.780839Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3 View Scree Plot to identify the number of components to be built\n",
    "plt.figure(figsize=(12,7))\n",
    "plt.xlabel('Number of Components',fontsize=15)\n",
    "plt.ylabel('Variance Explained',fontsize=15)\n",
    "plt.title('Scree Plot',fontsize=15)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T06:29:44.513369Z",
     "start_time": "2020-04-09T06:29:42.260689Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 4 Apply PCA for the number of decided components to get the loadings and component output\n",
    "\n",
    "# Using scikit learn PCA here. It does all the above steps and maps data to PCA dimensions in one shot\n",
    "from sklearn.decomposition import PCA\n",
    "# NOTE - we are generating only 8 PCA dimensions (dimensionality reduction from 33 to 8)\n",
    "pca = PCA(n_components=8, random_state=123)\n",
    "df_pca = pca.fit_transform(df_num_scaled)\n",
    "df_pca.transpose() # Component output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T06:29:44.523343Z",
     "start_time": "2020-04-09T06:29:44.515365Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading of each feature on the components\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T06:29:44.610110Z",
     "start_time": "2020-04-09T06:29:44.525338Z"
    }
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dataframe of component loading against each field and identify the pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T07:54:06.847723Z",
     "start_time": "2020-04-09T07:54:06.824755Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pca_loading = pd.DataFrame(pca.components_,columns=list(df_num_scaled))\n",
    "df_pca_loading.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_loading.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's identify which features have maximum loading across the components.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(22, 10), facecolor='w', edgecolor='k')\n",
    "ax = sns.heatmap(df_pca_loading, annot=True, vmax=1.0, vmin=0, cmap='Blues', cbar=False, fmt='.2g', ax=ax,\n",
    "                 yticklabels=['PC0','PC1','PC2','PC3','PC4','PC5','PC6','PC7'])\n",
    "\n",
    "column_max = df_pca_loading.abs().idxmax(axis=0)\n",
    "\n",
    "for col, variable in enumerate(df_pca_loading.columns):\n",
    "    position = df_pca_loading.index.get_loc(column_max[variable])\n",
    "    ax.add_patch(Rectangle((col, position),1,1, fill=False, edgecolor='red', lw=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = pd.DataFrame(df_pca,columns=['pc_loan_repayment','pc_balance_loan','pc_interest_rate','pc_tenure',\n",
    "                                  'pc_pre_emi','pc_emi','pc_receiveable','pc_income'])\n",
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA (Categorical Fields & Principal Components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.concat([df_cat, df_pca], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(22,7))\n",
    "sns.boxplot(x='city',y='pc_loan_repayment', data=df_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above plots we observe that for Ahmedabad & Surat the loan repayment amount is comparatively lower than other cities. \n",
    "\n",
    "Loan repayment amount is highest for Chennai compared to other cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(22,7))\n",
    "sns.boxplot(x='city',y='pc_interest_rate', data=df_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interest rate is highest for Ahmedabad and lowest for Tirupur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(22,7))\n",
    "sns.boxplot(x='city',y='pc_pre_emi', data=df_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "City of Erode seems to have highest pre-emi payments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "\n",
    "- From above plots we observe that for Ahmedabad city, loan repayment amount is comparatively lower than other cities however, the interest rate charged to the customers is much higher\n",
    "\n",
    "- For Erode city, loan repayment amount is low, interest rate is comparative high and the pre-emi amount paid by customers from this city is also high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply rules & analyze patterns in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1 - Advance Closure\n",
    "Customer who \n",
    "- has to pay lesser loan, \n",
    "- has done some payment as part of pre-emi and \n",
    "- has an high income \n",
    "have higher chances to do *advance payment* and close the loan in advance thus causing loss to the financial institute\n",
    "\n",
    "Let's create a subset dataframe *df_adv_closure* which satisfies all above conditions and analyze it further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond1 = df_new.pc_loan_repayment < df_new.pc_loan_repayment.quantile(0.25) # customers paying lesser loan\n",
    "cond2 = df_new.pc_pre_emi > df_new.pc_pre_emi.quantile(0.75) # customers who have paid high pre-emi\n",
    "cond3 = df_new.pc_income > df_new.pc_income.quantile(0.75) # customers who have high income\n",
    "df_adv_closure = df_new[cond1 & cond2 & cond3] # Find customers satisifying all above conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adv_closure.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above index corresponds to the rows or loan accounts which satisfy the Case 1 conditions. Let's create a new field 'advance_closure' which will hold a value of 1 for above indexed rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adv_closure=df_adv_closure.assign(advance_closure=1)\n",
    "df_adv_closure.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge *advance_closure* field with our original dataframe for the selected index and analyze further "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = df.merge(df_adv_closure['advance_closure'],left_index=True,left_on=df.index, right_index=True, \n",
    "                    right_on=df_adv_closure.index, how='left')\n",
    "df_merge.head(252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge.advance_closure.fillna(0, inplace=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge.advance_closure.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze data for probable customers who may payoff the loan in advance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df_merge.advance_closure, df_merge.num_emi_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df_merge., df_merge.outstanding_principal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.(df_merge.advance_closure, df_merge.paid_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "\n",
    "28 customers have low outstanding principal, have paid less interest so far and we also see that there are more changes in emi value being paid. Hence, there is high probability that these customers may close the loans in advance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2 - Loan Default:\n",
    "\n",
    "Customers \n",
    "- with high loan, \n",
    "- low income and \n",
    "- high loan interest rate \n",
    "are probable customers to default.\n",
    "\n",
    "Let's create a subset dataframe *df_delay_closure* which satisfies all above conditions and analyze further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond1 = df_new.pc_loan_repayment > df_new.pc_loan_repayment.quantile(0.75)# customers paying higher loan\n",
    "cond2 = df_new.pc_income < df_new.pc_income.quantile(0.25) # customers with lesser income\n",
    "cond3 = df_new.pc_interest_rate > df_new.pc_interest_rate.quantile(0.75) # customers paying higher interest\n",
    "\n",
    "df_delay_closure = df_new[cond1 & cond2 & cond3]# Find customers satisifying all above conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delay_closure.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above index corresponds to the rows or loan accounts which satisfy the Case 2 conditions. Let's create a new field *'delay_closure'* which will hold a value of 1 for above indexed rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delay_closure = df_delay_closure.assign(delay_closure=1)\n",
    "df_delay_closure.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge the *df_delay_closure* with original dataframe *'df'* and just add the *'delay_closure'* field "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = df.merge(df_delay_closure['delay_closure'],left_index=True,left_on=df.index, right_index=True, \n",
    "                    right_on=df_delay_closure.index, how='left')\n",
    "df_merge.head(79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge.delay_closure.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge.delay_closure.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze data for probable customers who may default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.(df_merge.delay_closure, df_merge.net_ltv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df_merge., df_merge.outstanding_principal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df_merge.delay_closure, df_merge.paid_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
